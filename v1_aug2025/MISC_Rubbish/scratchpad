.  It would be of great interest to evaluate this model on more thorough data, with more levels in the categorical variables.  This would more accurately reflect the state of the wine market, as wines come from far and wide—notably the likes of Italy, France, Spain, Germany, Greece, Portugal, and even the islands of southwest Lake Erie, where fox grape wines are made.  The last thereof presents another data option: grape cultivar.

Wines are first and foremost classified by what cultivar of wine grapes is used, such as Pinot, Cabernet, Malbec, Grenache, and others.  Most wine grape cultivars are from one species, but fox grapes are native to America—a distant relative of the Old World species, with distinctly different flavor profiles.  The use of the fox grape is why Manischewitz (a port-sweet, affordable, and Shabbat-suitable concord wine) and Concord grape jelly taste so unique among grape products, and this makes for an interesting category.  The color category could certainly be extended beyond "white" and "red"; we could add rosé and sparkling, both being different from whites and reds in their methods of manufacture just as whites and reds differ.

Additionally, further investigation of the mathematical reasons for the scale of the data having the effect it does on the presence of NaNs in the trained model.  Another route of further investigation is the ideal method of data standardization for the numerical data, and how various methods affect the real-world implications of the data—for example, whether a variable's physical or chemical nature should be or can permissibly be represented in a logarithmic way, or if that kind of transformation, based on data distribution alone, could warp the mathematical effect of the variable on the model or if it may instead say something about the sampling of the data which could be improved or the decisions of those generating the quality ratings in how they select wines eligible to test (do they preclude fox grape wines due to prevailing ideas in the industry about the possible quality such grapes can yield?).

In any event, regardless of the quirks of the data, the model we created had as little as a 6.2% error rate in testing using the Training Data for a train-test split, and assuming the distribution of the data in the Testing Data is the same as Training Data, the model should perform very well in its current form.